## مدل‌های زبانی عمیق

در بخش قبل با شبکه‌های ترنسفورمری آشنا شدید. همچنین پیشتر با شبکه‌های بازگشتی آشنا شدید و مسائلی نظیر دسته‌بندی متون و دسته‌بندی توکن را حل کردید. اگر مدل‌های زبانی آماری را با این شبکه‌های عصبی ادغام کنیم آن‌گاه مدل‌های زبانی عمیق حاصل می‌شود. مدل‌های زبانی عمیق برخلاف مدل‌های زبانی آماری،نیاز به محدود کردن متن زمینه برای پیش‌بینی ندارد و می‌تواند از تمامی متن ورودی برای پیش‌بینی یک توکن جدید استفاده کند. این مدل‌ها معمولا به صورت خود-نظارتی (self-supervised) آموزش می‌بیند. نشان داده شده است که این مدل‌های زبانی بهترین عملکرد را در تسک پیش‌بینی کلمات داشته‌اند. همچنین در این مدل‌ها این قابلیت وجود دارد که بتوان سایر تسک‌های NLP را نیز پیشرفت داد و دقت و عملکرد آن‌ها را بسیار ارتقا داد. 

برای آن که بتوان در سایر تسک‌های NLP از مدل‌های زبانی عمیق استفاده کر، باید مدل از پیش‌اموزش داده شده‌ (pretrained) زبانی را بر روی مدل تسک مد نظرمان تنظیم دقیق (fine-tune) کنیم. از بین تمامی مدل‌های زبانی عمیق، مدل‌های زبانی مبتنی بر شبکه‌های عصبی ترنسفورمر توانایی بیشتری برای تنظیم-دقیق دارند.

به طور کلی تمامی مدل‌های زبانی عمیق را می‌توان در دیاگرام زیر خلاصه کرد:

![](./deep_lm.png)



از بین تمامی انواع مدل‌های زبانی، مدل‌های زبانی خود-رگراسیون (auto-regressive)  مدل‌های زبانی خودکدگذار (auto-encoder) برای ما اهمیت بسیار زیادی دارد. مدل زبانی GPT یک مدل زبانی خود-رگراسیون و مدل زبانی BERT یک مدل زبانی خودکدگذار است.



### مدل‌های زبانی خود-رگراسیون (auto-regressive)

در این مدل‌های زبانی، برای پیش‌بینی هر توکن، از تمامی توکن‌های پیشین استفاده می‌گردد. از جمله شاخص‌ترین این مدل‌ها، مدل GPT است که از بخش کدگشا (decoder) مدل‌های ترنسفورمر استفاده می‌کند. به عنوان نمونه برای جمله‌ی "علی به مدرسه رفت" می‌توان شبکه عصبی مربوط به یک مدل خود-رگراسیون را به صورت زیر در نظر گرفت:

![](auto-regressive.png)

رابطه احتمالاتی این مدل‌ها نیز به صورت زیر است:

$$P(x;\theta )=\prod_{t=1}^{T} P(x_t | x_{< t} ; \theta)$$



که در این رابطه $\theta$ پارامترهای مدل و  $x_{< t}$ نشان‌دهنده تمامی توکن‌های قبل از توکن $x_t$ می‌باشد. 

برای آشنایی بیشتر با این مدل‌های زبانی [این مقاله](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) را مطالعه کنید.



## مدل‌های زبانی خودکدگذار (autoencoder)

در مسائل فهم زبان طبیعی (Language Understanding) اغلب از این مدل‌های زبانی استفاده می‌شود. در مدل‌های زبانی خود-رگراسیونی، برای پیش‌بینی یک کلمه جدید، در هر لحظه صرفا به یک طرف از متن زمینه دسترسی وجود دارد. اما در مدل‌های زبانی خودکدگذار، در پیش‌بینی هر کلمه، در هر لحظه به تمامی متن ورودی (به جز بخش‌های پوشانده شده) دسترسی وجود دارد. مشهورترین مدل زبانی خودکدگذار، مدل زبانی BERT است که از بخش کدگذار ترنسفورمر استفاده می‌کند. این مدل از تکنیکی تحت عنوان مدل زبانی پوشانده شده (Masked Language Modeling) استفاده می‌کند. در این روش بخشی از توکن‌های جمله ورودی پوشانده می‌شود و سپس با استفاده از کدگذار ترنسفورمر، بخش‌های پوشانده شده بازسازی می‌شوند. یک مثال از این مدل را در شکل زیر مشاهده می‌کنید.

![](autoencoder.png)



رابطه این مدل‌های زبانی به صورت زیر است:

$$P(x;\theta )=\prod_{m \in M} P(x_m | x \in \bar{M} ; \theta)$$



که در این رابطه $M$ مجموعه تمامی توکن‌های پوشانده شده است و به تبع $\bar{M}$ مجموعه تمامی توکن‌های موجود است.

برای آشنایی بیشتر با این مدل‌ها، [این مقاله](https://arxiv.org/pdf/1810.04805.pdf) را مطالعه کنید.



## کتابخانه Huggingface

برای کار با مدل‌های زبانی عمیق، نیاز است تا با کتابخانه Huggingface آشنا شوید. در این کتابخانه می‌توانید پیاده‌سازی، مدل‌های تنظیم دقیق شده  و نحوه استفاده از تمامی مدل‌های زبانی مشهور مبتنی بر ترنسفورمر را پیدا کنید. برای آشنایی با این کتابخانه به [این لینک](https://huggingface.co/docs/transformers/main/) مراجعه کنید. 





> تمرین1: با استفاده از کتابخانه Huggingface مدل زبانی GPT2 را بر روی زبان فارسی راه‌اندازی کنید و 10 متن تصادفی تولید کنید.





> تمرین2: با استفاده از کتابخانه  Huggingface مدل ParsBERT را به عنوان مدل Pre-train استفاده کنید و بر روی تسک تحلیل احساسات که از [این لینک]() قابل دسترس است، fine-tuning انجام دهید.







