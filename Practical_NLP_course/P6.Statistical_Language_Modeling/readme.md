## مدل‌های زبانی آماری

یکی از عملیات‌ها و کارهای پایه‌ای و اساسی در حوزه‌ی هوش مصنوعی و به خصوص پردازش زبان طبیعی طراحی یک مدل زبانی می‌باشد.  این مسئله در واقع یک مسئله‌ی پیش‌بینی داده‌ی دنباله‌ای می‌باشد. به این معنی که عنصر بعدی دنباله را با داشتن عناصر قبلی دنباله پیش‌بینی می‌کند. چون در جملات، کلمات و کاراکترهای بعدی به کلمات و کاراکترهای قبلی وابسته هستند می‌توان جمله را به عنوان یک داده‌ی دنباله‌ای در نظر گرفت. به مدلی که بتواند کلمات و یا کاراکترهای بعدی را با استفاده از کلمات و کاراکترهای قبلی پیش‌بینی کند مدل زبانی گفته می‌شود. البته مدل‌های زبانی تعاریف دیگری نیز دارند که در بخش‌های بعد به آن‌ها می‌پردازیم.

مدل‌های زبانی انواع گوناگونی دارند. از جمله ساده‌ترین و پایه‌ای‌ترین مدل‌های زبانی، مدل‌های زبانی n-gram هستند که مبتنی بر روابط آماری شکل گرفته‌اند. به این مدل‌های زبانی مدل‌های زبانی آماری نیز می‌گویند. در این جا به این مدل‌های زبانی می‌پردازیم.

. در مدل‌های زبانی *n*-gram احتمال رخداد $m$ عبارت $t_1,...,t_m$ به صورت یک دنباله در کنار یکدیگر را مطابق فرض مارکوفی می‌توان به صورت رابطه ‏2‑21 نشان داد (واضح است که $m>n$). توجه داشته باشید که در این نوع مدل زبانی باید احتمال رخداد $n$ کلمه‌ی نخست را از قبل داشته باشیم.

$$P(t_1,...,t_m) = P(t_1) \prod_{i=2}^{m} P(t_i|t_1,...,t_{i-1}) \approx P(t_1,...,t_n) \prod_{i=n+1}^{m} P(t_i|t_{i-n},...,t_{i-1})$$

فرض ساده‌سازی استفاده شده در این مدل‌های زبانی باعث می‌شود تا پیش‌بینی توکن (کلمه) بعد در یک جمله، صرفا به $n$ توکن (کلمه) قبلی نگاه کنیم. که این یک نقطه ضعف در مدل‌های زبانی n-gram است. 

اگر در این مدل‌ها $n=0$ آنگاه به آن‌ها Zerogram، اگر $n=1$ به آن‌ها Unigram، اگر $n=2$ به آن‌ها Bigram و اگر $n=3$ به آن‌ها Trigram می‌گویند.

یکی از مشکلات اساسی در هنگام استفاده از مدل‌های زبانی این است که با $n$ توکن کنار یکدیگر مواجه شویم که آن را در مجموعه دادگان مشاهده نکرده‌ایم. در این صورت ساده‌ترین کار این است که احتمال رخداد این $n$ توکن در کنار یکدیگر را برابر صفر قرار دهیم. اما این کار درستی نیست زیرا ممکن است که ما یک $n$تایی مشابه را در مجموعه دادگان داشته باشیم که دقیقا با $n$تایی داده شده یکسان نیست. به همین سبب روش‌هایی ارائه شدند تا این مشکل را حل کنند. به کلیه‌ی این روش‌ها که مشکل صفر شدن احتمال را حل می‌کنند روش‌های Smoothing  می‌گویند. تعدادی از این روش‌های Smoothing  در زیر لیست شده‌اند:

- Laplace Smoothing
- Bayesian Smoothing with Dirichlet Prior
- Absolute Discounting
- Kneser-Ney Smoothing
- Pitman-Yor Processes

برای آشنایی با روش‌های Smoothing [این لینک](https://vitalflux.com/quick-introduction-smoothing-techniques-language-models/) را مطالعه کنید.

