<div dir="rtl" align='right'>

در این بخش به مرور مباحث یادگیری ماشین می‌پردازیم. پیش‌فرض ما در این جا این است که شما با این مباحث آشنایی دارید و این مباحث صرفا جنبه مرور و یادآوری دارد.

به طور کلی اگر بخواهیم یک دسته‌بندی از هوش مصنوعی ارائه دهیم می‌توانیم به شکل زیر اشاره کنیم:

![](AI_field.png)

به طور کلی یادگیری ماشین یک زیرمجموعه از هوش مصنوعی است که با سایر حوزه‌های هوش مصنوعی ارتباط تنگاتنگی دارد. اگر بخواهیم یک هدف کلی برای یادگیری ماشین مشخص کنیم می‌توانیم بگوییم که کار یادگیری ماشین تخمین تابع است. در واقع در حوزه یادگیری ماشین تلاش داریم تا روابط بین پدیده‌ها را با یک تابع مدل کنیم. این تابع می‌تواند با دریافت یک یا چند ورودی، یک یا چند خروجی را برای ما تولید کند.

اگر بخواهیم یک نگاه کلی به یادگیری ماشین بیندازیم و آن را با برنامه‌نویسی عادی مقایسه کنیم، می‌توانیم بگوییم که در برنامه‌نویسی، برنامه و داده ورودی به ماشین داده می‌شود و ماشین با اجرای برنامه بر روی داده ورودی، داده خروجی را تولید می‌کند. اما در یادگیری ماشین، نتوانیم برای تولید داده خروجی از ورودی برنامه‌ای بنویسیم. لذا هم داده ورودی و هم داده خروجی به ماشین داده می‌شود و از این جا به بعد، وظیفه‌ی به دست آوردن برنامه مناسب برای تولید خروجی از ورودی به عهده ماشین خواهد بود. به شکل زیر توجه کنید.

![](com.png)



اما برای این که ماشین بتواند رابطه بین ورودی و خروجی را تشخیص دهد باید روش‌ها و الگوریتم‌هایی را به ماشین بدهیم که بتواند رابطه بین ورودی و خروجی را تشخیص دهد. برخی از مهم‌ترین این الگوریتم‌ها در زیر لیست شده‌اند:

- Linear Regression

- Logistic Regression

- K-Nearset Neighbors (KNN)

- Support Vector Machine (SVM)

- Naive Bayes

- Decision Tree

- Random Forest

- Single Perceptron

- Neural Networks

- K-means

- Principal Component Analysis (PCA)


از بین این الگوریتم‌ها، 9 مورد ابتدایی را باناظر (supervised) گویند و مابقی را بدون‌ناظر یا (Unsupervised) گویند. در الگوریتم‌های باناظر بدین صورت عمل می‌شود که زوج داده ورودی و خروجی (یعنی ${(x,y) \in X \times Y}$) همزمان در دسترس ما است و مدل برای آموزش از آن استفاده می‌کند. اما در الگوریتم‌های بدون‌ناظر فقط داده ورودی در دسترس است و مدل هیچ‌گونه بازخوردی بابت خروجی‌ای که در هنگام آموزش تولید می‌کند دریافت نمی‌کند. 

اکنون به بررسی اجمالی چند مورد از این الگوریتم‌ها خواهیم پرداخت. 

## الگوریتم رگرسیون خطی

در الگوریتم رگرسیون خطی یک مدل خطی نظیر $\hat{y} = w.x + b$ با ضرایب $w=(w_1, w_2 ,..., w_k) \in \mathbb{R}^k$ و بایاس $b \in \mathbb{R}$ به دادگان برازش می‌شود. برای یافتن ضرایب و بایاس بهینه کافی است تا ضرایب تا تابع خطا زیر را بهینه  کنیم:

$$\ell = \frac{1}{n-1} (y-\hat{y})^2 $$

که در رابطه فوق $n$ تعداد نمونه‌ها،  $y$ خروجی واقعی متناسب با ورودی $x$  و $\hat{y}$ خروجی پیش‌بینی شده مدل می‌باشد. برای پیاده‌سازی این بخش از کتابخانه sklearn پایتون استفاده خواهیم کرد. برای اطلاعات بیشتر به [این لینک](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) مراجعه کنید. 





> تمرین 1: تابع $y = sin(x) \;  0 \leq x \leq \frac{\pi}{2}$ را در نظر بگیرید. با استفاده از کتابخانه sklearn یک مدل رگرسیون خطی بر این تابع برازش کنید و خطای مدل خود را گزارش کنید.



## الگوریتم k نزدیک‌ترین همسایه (KNN)

برخلاف مدل رگرسیون خطی که یک مدل برای برازش توابع است، این الگوریتم یک دسته‌بندی است. این الگوریتم بدین صورت عمل می‌کند که فاصله یک داده ورودی را از تمامی دادگان ورودی موجود در مجموعه داده محاسبه می‌کند و سپس از بین k نمونه‌ای که از همه به داده ورودی جدید نزدیک‌تر هستند، کلاس‌های متناظر آن‌ها را در نظر می‌گیرد و بین آن کلاس‌ها رای‌گیری انجام می‌دهد و کلاسی که بیشترین تکرار را در بین این k همسایه داشته باشد به عنوان خروجی در نظر گرفته می‌شود. این الگوریتم به عنوان یک الگوریتم تنبل شناخته می‌شود، زیرا فاز آموزش ندارد و هر مرتبه باید تمام دادگان را اسکن کند.



> تمرین2: دادگان دو کلاسه زیر را در نظر بگیرید:
>
> $x_1  = (1,1) $, $y_1 = 1$
>
> $x_2  = (0,1) $, $y_2 = 1$
>
> $x_3  = (2,4) $, $y_3 = -1$
>
> $x_4  = (-2,1) $, $y_4 = -1$
>
> $x_5  = (1,3) $, $y_5 = 1$
>
> $x_6  = (9,1) $, $y_6 = -1$
>
> اکنون با استفاده از الگوریتم KNN برای k=3 هم به صورت دستی و هم با استفاده از کتابخانه sklearn  کلاس متناظر به داده $x=(0, 0)$ را بیابید. برای آشنایی با نحوه آموزش مدل KNN با استفاده از کتابخانه sklearn  به [این لینک](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) مراجعه کنید.



## الگوریتم بیز ساده (Naive Bayes)

الگوریتم بیز ساده همانند الگوریتم KNN یک دسته‌بند است. این الگوریتم مبتنی بر قانون بیز در احتمال طراحی شده است. اگر داده ورودی $x=(x_1, x_2, ..., x_n)$ در دسترس ما باشد و مسئله ما نیز شامل کلاس‌های $C_1, ..., C_k$ باشد، آنگاه برای این که مشخص شود داده ورودی $x$ به کدام کلاس تعلق دارد کافی است تا احتمال شرطی $P(C_i|x)$ که $i \in {1, ..., k}$ محاسبه شود. اکنون کافی است تا کلاس متناظر با بیشترین احتمال شرطی را انتخاب کنیم و به عنوان کلاس خروجی در نظر بگیریم. یعنی $C^* = \underset{i=1,..,k}{argmax P(C_i|x)}$.

اما بخش اصلی در اینجا محاسبه $P(C_i|x)$ می‌باشد. واضح است که نمی‌توان این احتمال را به صورت مستقیم از دادگان تخمین زد. به همین سبب از قاعده بیز در احتمال به صورت زیر کمک می‌گیریم:

 $$P(C_i|x) = \frac{P(x|C_i) P(C_i)}{P(x)}$$

از آن جایی که $P(x)$ یک مقدار ثابت است و از آن‌جایی که محاسبه آن از طریق مجموعه دادگان به سادگی قابل انجام نیست لذا می‌توان احتمال شرطی فوق را به صورت زیر و با یک نسبت از احتمال اصلی نوشت:

$$P(C_i|x) \propto P(x|C_i) P(C_i)$$

همچنین از آنجایی که $x=(x_1, ..., x_n)$ لذا محاسبه $P(x|C_i)$ به سادگی قابل انجام نیست. لذا  برای انجام این کار از فرض ساده‌سازی زیر استفاده می‌کنیم:

$$P(x|C_i) = \prod_{j=1}^{n} P(x_j|C_i)$$



## الگوریتم K-means

این الگوریتم یک الگوریتم خوشه‌بندی است و در واقع یک الگوریتم بدون‌ناظر است. این الگوریتم دارای یک پیش‌فرض اولیه است و این پیش‌فرض نیز این است که k خوشه داریم و می‌خواهیم کلیه دادگان را در این $k$  خوشه افراز کنیم. 

این الگوریتم بدین صورت عمل می‌کند که ابتدا $k$ داده فرضی را به عنوان مراکز خوشه‌ها، ابتدا به صورت تصادفی مقداردهی اولیه می‌کند و سپس فاصله‌ی  هر یک از نمونه‌های موجود در دادگان را تا هر خوشه محاسبه می‌کند. سپس هر نمونه را به خوشه‌ای اختصاص می‌دهد که از آن کمترین فاصله را دارد. اکنون با استفاده از نمونه‌های موجود در هر خوشه یک مرکز خوشه جدید محاسبه می‌شود. هر مرکز خوشه جدید در واقع میانگین تمامی نمونه‌های موجود در آن خوشه خواهد. اکنون مجدد فاصله هر مرکز خوشه تا هر نمونه محاسبه می‌شود و هر خوشه شامل نمونه‌هایی خواهد بود که از آن مرکز کمترین فاصله را دارند. این فرایند تا جایی ادامه پیدا می‌کند که هیچ مرکز خوشه‌ای تغییر پیدا نکند.

برای فهم بهتر الگوریتم به [این ویدئو](https://www.youtube.com/watch?v=hDmNF9JG3lo) از andrew ng  مراجعه کنید.



تمرین3: دادگان زیر را با استفاده از الگوریتم K-means در دو خوشه، خوشه‌بندی کنید.  :

$x_1  = (1,1) $

$x_2  = (0,1) $

$x_3  = (2,4) $

$x_4  = (-2,1) $

$x_5  = (1,3) $

$x_6  = (9,1) $

توجه کنید که این کار هم باید به صورت دستی و هم با پیاده‌سازی برنامه در پایتون انجام شود. برای سهولت می‌توانید از کتابخانه sklearn  در [این لینک](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) استفاده کنید.



## الگوریتم تحلیل مولفه اساسی (PCA)

در الگوریتم تحلیل مولفه اساسی هدف این است که ابعاد دادگان را کاهش داده و تعداد ویژگی‌های موجود در دادگان را کاهش دهیم. نکته مهم در این الگوریتم این است که پس از اعمال آن به دادگان، ماهیت تمامی ویژگی‌های موجود در دادگان تغییر می‌کند. به عبارت دیگر، پس از اعمال این الگوریتم به دادگان، کلیه‌ی ویژگی‌ها تغییر می‌کنند و دیگر ویژگی‌های سابق نخواهند بود.

اگر در مجموعه دادگان مورد استفاده، $n$ ویژگی وجود داشته باشد، الگوریتم PCA می‌تواند دقیقا $n$ ویژگی با ماهیت جدید برای تولید کند، با این تفاوت که ویژگی‌ها بر اساس درجه اهمیتی که دارند مرتب شده‌اند. اکنون می‌توانیم $k$ ویژگی با درجه اهمیت کم را حذف کنیم و صرفا $n-k$ ویژگی با درجه اهمیت بیشتر را نگه داریم.

برای تفسیر این الگوریتم براساس نمادهای ریاضی، فرض کنید $X \in \mathbb{R}^{n \times d}$ یک ماتریس است که در برگیرنده تمامی دادگان ما است. این ماتریس به گونه‌ای است که در ستون‌های آن، نمونه‌ها و در سطرهایش، ویژگی‌ها قرار گرفته‌اند. ابتدا از ماتریس $X$ میانگین ستونی می‌گیریم و میانگین به دست آمده را از تک تک ستون‌های ماتریس $X$ حذف می‌کنیم تا میانگین این ماتریس صفر شود. فرض کنید ماتریس جدید به دست آمده $\hat{X}$ باشد. اکنون به صورت زیر ماتریس ماتریس کوواریانس را محاسبه می‌کنیم:

$$C = \frac{1}{n-1} \hat{X}^T\hat{X}$$ 

اکنون ماتریس $C$ را با استفاده از تجزیه مقدار ویژه به صورت زیر تجزیه می‌کنیم:

$$C = U\Lambda U^T = (U\Lambda^(1/2))(U\Lambda^(1/2))^T$$ 

که در این رابطه ماتریس $U$ ماتریس‌متعامد هستند. توجه کنید که ماتریس متعامد ماتریسی است که ضرب آن در تنهاده‌اش (از سمت راست، چپ یا هر دو طرف) برابر ماتریس همانی شود.

 همچنین ماتریس $\Lambda$ یک ماتریس قطری است. 

اکنون عناصر روی قطر ماتریس $\Lambda$ مشخص‌کننده میزان اهمیت هر یک از $n$ ویژگی جدید به دست آمده است. این مقادیر از درجه اهمیت زیاد به کم روی قطر مرتب شده‌اند. 

لذا اگر بخواهیم $k$ ویژگی را کنار بگذاریم و $n-k$ ویژگی را انتخاب کنیم، کافی است تا $k$تا از سطرها و ستون‌های آخر دو ماتریس فوق را حذف کرده و تبدیل زیر را بسازیم:

$$T = U_{n-k} \Lambda_{n-k}$$

اکنون به ازای هر نمونه جدید، می‌توانیم $n-k$ 